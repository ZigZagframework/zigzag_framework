# ZIGZAG

- Environment: python 2.7, python 3.6, tigress 3.1
- Download link for tigress: [tigress.cs.arizona.edu/download.html](http://tigress.cs.arizona.edu/download.html)
- Dataset: In `ZigZag/Dataset`

## Step1: Code transformation (Python 2.7)

### SARD Dataset

Code is in `ZigZag-Framework/code_transform/sard-deform`, Dataset is in `Dataset\train_programs` and `Dataset\target_programs`

1. getVulLineForCounting.py: This file is used to extract the line numbers of vulnerable lines from SARD_testcaseinfo.xml
2. labelFile.py: This file is used to add 'test_insert();' in the source code to mark the vulnerable lines.
3. encode_trans.py: This file is uesd to do code transformation for SARD dataset
4. new_label_con.py: This file is used to get the vulnerable lines in transformed code.

### NVD Dataset

Code is in `ZigZag-Framework/code_transform/nvd-deform`, Train dataset is composed of all the CVE until 2016. Test dataset is composed of all the CVE after 2016 (2017 - 2019)

1. download_software.py: This file is used to download software sourcecode related to specific CVE ID for code transformation.
2. get_vulline.py: This file is used to extract the line numbers of vulnerable lines from diff files.
3. label_nvd.py: This file is used to add 'test_insert();' in the source code to mark the vulnerable lines.
4. getTigressFunc2.py: This file is uesd to do code transformation for NVD dataset.
5. new_label_con.py: This file is used to get the vulnerable lines in transformed code.

## Step 2: Generating code fragments (Python 2.7)

For example, generate program slices by [SySeVR](https://github.com/SySeVR/SySeVR).

Code is in `SySeVR/Implementation`

1. batch_SARD.sh: A batch script for SARD dataset.
2. batch_NVD.sh: A batch script for NVD dataset.

## Step 3: Data preprocess (Python 3.6)

1. make_label.py: This file is used to get labels of slices. The input is slice file generated by extract_df.py and label file generated by
2. create_hash.py: This file is used to get the hash value of slices. The input is slice file generated by extract_df.py, and the output is hash list of slices.
3. delete_list.py: This file is used to get index of slices that need to be delete. The input is hash list generated by create_hash.py,and the output is a list contains the index.
4. process_dataflow_func.py: This file is used to process the slices, including read the pkl file (both labels and delete list) and split codes into corpus. Meanwhile this file splits the dataset by different types (original data or tigress data) and different testcase. The inputs are the slice file and the label file generated by extract_df.py and make_label.py, and the output is the corpus file named with "slice index"-"focus index".

## Step 4: Deep learning model

ZigZag\ZigZag-Framework\train_model

1.train_w2vmodel.py:Train a model to convert word segmentation into vectorget a word to vector mode

2.product_train_data.py:Generate training data using w2v Model converts files into trainable data of uniform shape

3.train.py:Entry to model training, setting parameters through the command line.

Reference command line

```
CUDA_VISIBLE_DEVICES=0 nohup python -u train.py --modelKind 'BGRU' --predThreshold 0.4 --epochTimes '35,2,2,2' --learningRate '0.0012,0.001,0.0003,0.0001' --trainTimes 100 > 30,2,2,2,0.0012,0.001,0.0003,0.0001.txt  2>&1 &



```
